{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs236299-2023-spring/lab4-1-joannj35/blob/master/lab4_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# my files are in 'labs/lab0-0'\n",
        "!cp -r /content/drive/MyDrive/NLP_labs/lab4-1/* .\n",
        "!pip install -r requirements.txt\n",
        "# restart the runtime\n",
        "import os\n",
        "os._exit(00)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr-kLfZ0pf7a",
        "outputId": "1c82e894-a757-41bb-f5b1-26ffd3bc240b"
      },
      "id": "tr-kLfZ0pf7a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting otter-grader==1.0.0 (from -r requirements.txt (line 1))\n",
            "  Downloading otter_grader-1.0.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m164.0/164.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nltk==3.5. (from -r requirements.txt (line 2))\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wget (from -r requirements.txt (line 3))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 1)) (5.9.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 1)) (7.34.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 1)) (6.5.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 1)) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 1)) (67.7.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 1)) (6.3.1)\n",
            "Collecting docker (from otter-grader==1.0.0->-r requirements.txt (line 1))\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from otter-grader==1.0.0->-r requirements.txt (line 1)) (3.1.2)\n",
            "Collecting dill (from otter-grader==1.0.0->-r requirements.txt (line 1))\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfkit (from otter-grader==1.0.0->-r requirements.txt (line 1))\n",
            "  Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
            "Collecting PyPDF2 (from otter-grader==1.0.0->-r requirements.txt (line 1))\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.5.->-r requirements.txt (line 2)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.5.->-r requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from nltk==3.5.->-r requirements.txt (line 2)) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker->otter-grader==1.0.0->-r requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->otter-grader==1.0.0->-r requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->otter-grader==1.0.0->-r requirements.txt (line 1)) (1.26.16)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker->otter-grader==1.0.0->-r requirements.txt (line 1)) (1.6.0)\n",
            "Collecting jedi>=0.16 (from ipython->otter-grader==1.0.0->-r requirements.txt (line 1))\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->otter-grader==1.0.0->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (4.9.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (5.3.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.2.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (1.2.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader==1.0.0->-r requirements.txt (line 1)) (2.17.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->otter-grader==1.0.0->-r requirements.txt (line 1)) (4.3.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader==1.0.0->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader==1.0.0->-r requirements.txt (line 1)) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->otter-grader==1.0.0->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.8.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r requirements.txt (line 1)) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.19.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (3.7.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (6.1.12)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->otter-grader==1.0.0->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->otter-grader==1.0.0->-r requirements.txt (line 1)) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->otter-grader==1.0.0->-r requirements.txt (line 1)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->otter-grader==1.0.0->-r requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert->otter-grader==1.0.0->-r requirements.txt (line 1)) (23.2.1)\n",
            "Building wheels for collected packages: nltk, wget\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=1ae198a1dad47a1a5a272718338177cfd4514cebf1c6826c617ee68237ce9c36\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/ab/82/f9667f6f884d272670a15382599a9c753a1dfdc83f7412e37d\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=f6f52d6f34ded256e2a1f4b252248860abbbaffd527ad558c1bb123f6371d913\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built nltk wget\n",
            "Installing collected packages: wget, pdfkit, PyPDF2, nltk, jedi, dill, docker, otter-grader\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed PyPDF2-3.0.1 dill-0.3.6 docker-6.1.3 jedi-0.18.2 nltk-3.5 otter-grader-1.0.0 pdfkit-1.0.0 wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "14c90399",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14c90399",
        "outputId": "4163e8b9-6c09-4292-b12a-e7dfe4d75f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "\n",
        "       Prints the result to stdout and returns the exit status.\n",
        "       Provides a printed warning on non-zero exit status unless `warn`\n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2023-spring/lab4-1.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8ed136c6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8ed136c6"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4640c82",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "f4640c82"
      },
      "source": [
        "# Course 236299\n",
        "## Lab 4-1 - First-order logic, lambda calculus, semantic parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c96b71",
      "metadata": {
        "id": "c6c96b71"
      },
      "source": [
        "In this lab, you'll use first-order logic (FOL), augmented by the lambda calculus of functions, as the formal representation of meaning for a language in the by-now-familiar air travel (ATIS) domain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7adf717",
      "metadata": {
        "id": "c7adf717"
      },
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b799e07d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b799e07d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pprint\n",
        "import sys\n",
        "import wget\n",
        "\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1067d5f7",
      "metadata": {
        "id": "1067d5f7"
      },
      "outputs": [],
      "source": [
        "# Download code for augmented grammars\n",
        "remote_script_dir = \"https://raw.githubusercontent.com/nlp-236299/data/master/scripts/\"\n",
        "local_script_dir = \"./scripts/\"\n",
        "\n",
        "# Create and search the local script directory\n",
        "os.makedirs(local_script_dir, exist_ok=True)\n",
        "sys.path.insert(1, local_script_dir)\n",
        "\n",
        "# Download files to script directory\n",
        "wget.download(remote_script_dir + \"trees/transform.py\", out=local_script_dir)\n",
        "\n",
        "# Import functions for transforming augmented grammars\n",
        "import transform as xform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b331a5db",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b331a5db"
      },
      "source": [
        "# First-order logic\n",
        "\n",
        "Recall that FOL formulas are composed of constants, variables, predicates over them, logical operators over the predicates, and quantifiers.\n",
        "\n",
        "A first-order model describes what the constants denote, and what values the predicates are true of. For example, a model of the flight world, might consist of the following:\n",
        "\n",
        "* **Constants**:\n",
        "    * (places) Boston, New York, Tel Aviv\n",
        "    * (times) Morning, Evening\n",
        "    * (flights) DL10, DL11, DL13, LY01, LY12\n",
        "    \n",
        "* **Predicates**:\n",
        "    * *Flight*: DL10, DL11, DL13, LY01, LY12\n",
        "    * *Origin*:  (DL10, Boston), (DL11, Boston), (DL13, New York), (LY01, Tel Aviv), (LY12, New York)\n",
        "    * *Destination*:  (DL10, New York), (DL11, Tel Aviv), (DL13, Boston), (LY01, New York), (LY12, Tel Aviv)\n",
        "    * *DepartureTime*:  (DL10, Morning), (DL11, Evening), (DL13, Evening), (LY01, Evening), (LY12, Morning)\n",
        "    * *ArrivalTime*:  (DL10, Morning), (DL11, Morning), (DL13, Evening), (LY12, Evening)\n",
        "\n",
        "Using this model, we can express propositions such as \"DL13 is a flight\" and \"DL13 departs in the evening\" with formulas of FOL:\n",
        "* Flight(DL13)\n",
        "* DepartureTime(DL13, Evening)\n",
        "\n",
        "We can also combine expressions using Boolean operators to express statements like \"DL13 departs from Boston in the evening\":\n",
        "* DepartureTime(DL13, Evening) $\\land$ Origin(DL13, Boston)\n",
        "\n",
        "We use variables to refer to objects that are not specified. Variables are quantified to either express the existence of an object or to refer to all objects:\n",
        "\n",
        "\\begin{align*}\n",
        "1. \\ \\ &\\exists x. Flight(x) \\land Origin(x, Boston)  \\\\\n",
        "2. \\ \\ &\\forall x. Flight(x) \\implies Origin(x, Boston)\n",
        "\\end{align*}\n",
        "\n",
        "Write a plain English description of the meaning (a \"gloss\") for each of the two expressions above:\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: gloss_samples\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "279f334b",
      "metadata": {
        "id": "279f334b"
      },
      "outputs": [],
      "source": [
        "# TODO -- Provide an English gloss in the form of a string for each formula\n",
        "gloss1 = \"there exists a flight which originates from Boston\"\n",
        "gloss2 = \"all flights originate from Boston\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "12976542",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "12976542",
        "outputId": "d4c1c7ae-75a5-419f-ad71-099e6b92f659"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "grader.check(\"gloss_samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d0219a0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9d0219a0"
      },
      "source": [
        "Determine the truth values of the above propositions under the model of the flight world given above:\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: truth_samples\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3110c244",
      "metadata": {
        "id": "3110c244"
      },
      "outputs": [],
      "source": [
        "# TODO -- Fill in the truth values for the two propositions as Python booleans\n",
        "statement1 = True\n",
        "statement2 = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "73e0d39a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "73e0d39a",
        "outputId": "e8b76428-736e-444d-e23b-2e21e8d17423"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "grader.check(\"truth_samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c80def38",
      "metadata": {
        "id": "c80def38"
      },
      "source": [
        "## Implementing a model of the flight world\n",
        "We will need a Python implementation of the flight world. Our constants will be string objects:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "da056939",
      "metadata": {
        "id": "da056939"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "Boston = \"Boston\"\n",
        "NewYork = \"New York\"\n",
        "TelAviv = \"Tel Aviv\"\n",
        "\n",
        "DL10 = \"DL10\"\n",
        "DL13 = \"DL13\"\n",
        "LY01 = \"LY01\"\n",
        "LY12 = \"LY12\"\n",
        "DL11 = \"DL11\"\n",
        "\n",
        "Morning = \"Morning\"\n",
        "Evening = \"Evening\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29ede128",
      "metadata": {
        "id": "29ede128"
      },
      "source": [
        "Our predicates will be sets of objects (for unary predicates, which express _properties_) or tuples (for binary predicates, which express _relations_). We have defined some of the predicates below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9d2a136f",
      "metadata": {
        "id": "9d2a136f"
      },
      "outputs": [],
      "source": [
        "# Predicates\n",
        "\n",
        "# Properties\n",
        "Object = {Boston, NewYork, TelAviv, DL10, DL11, DL13, LY01, LY12, Morning, Evening}\n",
        "Flight = {DL10, DL11, DL13, LY01, LY12}\n",
        "# Relations\n",
        "Origin = {\n",
        "    (DL10, Boston),\n",
        "    (DL11, Boston),\n",
        "    (DL13, NewYork),\n",
        "    (LY01, TelAviv),\n",
        "    (LY12, NewYork),\n",
        "}\n",
        "Destination = {\n",
        "    (DL10, NewYork),\n",
        "    (DL11, TelAviv),\n",
        "    (DL13, Boston),\n",
        "    (LY01, NewYork),\n",
        "    (LY12, TelAviv),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd0b6a4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5dd0b6a4"
      },
      "source": [
        "Complete the definition of the predicates by defining `DepartureTime` and `ArrivalTime` according to the flight world given above. Assume that the first element of the tuple is the flight object and the second element is the time.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: finish_world\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e7b4c2a3",
      "metadata": {
        "id": "e7b4c2a3"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "DepartureTime = {\n",
        "    (DL10, Morning),\n",
        "    (DL11, Evening),\n",
        "    (DL13, Evening),\n",
        "    (LY01, Evening),\n",
        "    (LY12, Morning),\n",
        "}\n",
        "ArrivalTime = DepartureTime = {\n",
        "    (DL10, Morning),\n",
        "    (DL11, Morning),\n",
        "    (DL13, Evening),\n",
        "    (LY12, Evening),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7b910716",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "7b910716",
        "outputId": "33b39e09-af63-411e-9c38-4ca20717c597"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "grader.check(\"finish_world\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00a3de0d",
      "metadata": {
        "id": "00a3de0d"
      },
      "source": [
        "Given this implementation, we can determine truth values of simple FOL propositions. To check whether DL10 has an origin of Boston, that is, that the proposition $Origin(DL10, Boston)$ is true, we use set membership:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7f5bbab7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f5bbab7",
        "outputId": "a4dc6a44-5c96-41fb-a93d-c921c052ab74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "(DL10, Boston) in Origin"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53bb1895",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "53bb1895"
      },
      "source": [
        "Next, construct a Python expression that expresses the proposition that DL10 has an origin of Boston and a destination of New York. Name it `prop1`. We can then use it to test whether the proposition is true.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: dl10_endpoints\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2e48a453",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e48a453",
        "outputId": "fac83a00-154b-4924-baf1-2e6ee4d85254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# TODO -- Construct a Python expression that tests the proposition\n",
        "prop1 = (DL10, Boston) in Origin and (DL10, NewYork) in Destination\n",
        "print(prop1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "45763ed4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "45763ed4",
        "outputId": "f76293fa-5308-4bdb-9489-45658e347c91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "grader.check(\"dl10_endpoints\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1739915d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1739915d"
      },
      "source": [
        "# Lambda calculus\n",
        "The lambda calculus is a simple notation for expressing functions. By embedding FOL in the lambda calculus, it allows us to build functions over propositions of various sorts, and build up propositions from component parts by applying those functions.\n",
        "\n",
        "For example, $\\lambda x\\ldotp Origin(x,NewYork)$ is a function from entities to truth values, true just in case the entity is a thing (presumably a flight, though that isn't checked) whose origin is New York. This expression thus defines a _property_, the property of originating in New York.\n",
        "\n",
        "These lambda functions can be applied to arguments. Applying the expression above to $DL10$, gives us the expression $(\\lambda x\\ldotp Origin(x,NewYork)) (DL10)$. This can be simplified through the normal lambda calculus rules ([$\\beta$-reduction](https://en.wikipedia.org/wiki/Lambda_calculus#Î²-reduction)) to $Origin(DL10, NewYork)$, expressing the proposition that flight DL10 has New York as its origin. (As it turns out, that proposition is false. Nothing stops us from expressing falsehoods.)\n",
        "\n",
        "You may recognize the similarity between this $\\lambda$ notation, due to [Alonzo Church, inventor of the lambda calculus](https://en.wikipedia.org/wiki/Alonzo_Church), and [Python's syntax for defining anonymous functions](https://docs.python.org/3/reference/expressions.html#lambda), such as `lambda x: (x, NewYork) in Origin`. (This is not a coincidence, even though [Guido van Rossum is not a fan](https://www.artima.com/weblogs/viewpost.jsp?thread=98196).) The happy fact that Python already embeds a proper implementation of the lambda calculus means that we do not have to implement the lambda calculus ourselves. We'll just use Python's `lambda`.\n",
        "\n",
        "Given our implementation of the flight world, define Python `lambda` functions corresponding to the following glosses:\n",
        "\n",
        "1. Things that have a destination of New York\n",
        "1. Flights from Tel Aviv arriving in the evening\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: simple_lambda\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "3ffbdee8",
      "metadata": {
        "id": "3ffbdee8"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "expr1 = lambda x : (x, NewYork) in Destination\n",
        "expr2 = lambda x : x in Flight and (x, TelAviv) in Origin and (x, Evening) in ArrivalTime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "09dfede5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "09dfede5",
        "outputId": "cd5b53ab-95b3-44eb-e467-1e43206ee7fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "grader.check(\"simple_lambda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f7c97c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "00f7c97c"
      },
      "source": [
        "You can apply your functions on objects from the world and verify that you get the expected result. Use the expressions you just defined to check whether (1) flight DL10 has a destination of New York; and (2) whether flight LY01 is an evening-arriving flight from Tel Aviv.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: simple_lambda_2\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f8124ad3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8124ad3",
        "outputId": "6ac8d17a-b1ba-453b-fad6-d9ea23167f8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# TODO - check (1) and (2) using `expr1` and `expr2`\n",
        "result1 = expr1(DL10)\n",
        "result2 = expr2(LY01)\n",
        "print(result1)\n",
        "print(result2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2ae20483",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "2ae20483",
        "outputId": "34a96a7f-ab7b-4034-a7d9-fb9af6e3149e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "grader.check(\"simple_lambda_2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f3a11f3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6f3a11f3"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Notice that our flight world is underspecified, since flight LY01 does not have an arrival time. Is the result you got in `result2` desired? Would you suggest a different result?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_1\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56b12c90",
      "metadata": {
        "id": "56b12c90"
      },
      "source": [
        "one might think that this is not the desired result since we have missing information about the arrival time of flight LY01 (UNKNOWN). Perhaps a better result would be UNKNOWN symbol.\n",
        "in a more conservative approach we would rather output FALSE than risk a False negative answer. this totally depends on our implementation and what exactly we're seeking. ðŸ˜€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de42b1d",
      "metadata": {
        "id": "0de42b1d"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# Semantic parsing\n",
        "\n",
        "Semantic parsing (or semantic interpretation or analysis) is the task of converting a natural language sentence to a semantic representation, such as first-order logic. We will use a syntax-driven approach to semantic parsing, where context-free grammar rules are augmented with semantic augmentations, providing meanings for constituents derived by each rule. The meaning of each expression will be a function of the meaning of the expression's subconstituents. The meanings and the ways to combine them are defined in a syntactic-semantic grammar. For example, given a syntactic rule $A \\rightarrow B\\, C$, the meaning of $A$ is a function of the meaning of $B$ and the meaning of $C$.\n",
        "\n",
        "We want to be able to convert syntactic trees of natural language queries to FOL representations of the meanings of the associated queries. For example, given a sentence \"flights from Boston to New York\", we want to obtain the FOL expression $Flight(x) \\land Origin(x, Boston) \\land Destination(x, New York)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb124ab",
      "metadata": {
        "id": "fdb124ab"
      },
      "source": [
        "## An example grammar of flights\n",
        "\n",
        "Let us define a simple syntactic grammar for natural language queries about flights in our flight world. The grammar should cover NP expressions such as the following:\n",
        "\n",
        "* \"flights from Boston to New York\"\n",
        "* \"flights from Tel Aviv departing in the morning\"\n",
        "\n",
        "> In working with this grammar here and below, do not change the order of the productions in the grammar, as our unit tests depend on the order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "1c4c5787",
      "metadata": {
        "id": "1c4c5787"
      },
      "outputs": [],
      "source": [
        "grammar, _ = xform.parse_augmented_grammar(\"\"\"\n",
        "    NP -> 'flights'\n",
        "    NP -> NP PP\n",
        "\n",
        "    PP -> PP_PLACE\n",
        "    PP -> PP_TIME\n",
        "\n",
        "    PP_PLACE -> 'from' LOC\n",
        "              | 'leaving' LOC\n",
        "              | 'to' LOC\n",
        "              | 'arriving' 'at' LOC\n",
        "\n",
        "    PP_TIME -> 'arriving' TIME\n",
        "             | 'departing' TIME\n",
        "             | 'leaving' TIME\n",
        "\n",
        "    LOC -> 'Boston'\n",
        "    LOC -> 'New' 'York'\n",
        "    LOC -> 'Tel' 'Aviv'\n",
        "\n",
        "    TIME -> 'in' 'the' 'morning'\n",
        "    TIME -> 'in' 'the' 'evening'\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2ae06963",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ae06963",
        "outputId": "8ddee2af-ef5e-4b62-cbf9-3fdcc2e803e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar with 16 productions (start state = NP)\n",
            "    NP -> 'flights'\n",
            "    NP -> NP PP\n",
            "    PP -> PP_PLACE\n",
            "    PP -> PP_TIME\n",
            "    PP_PLACE -> 'from' LOC\n",
            "    PP_PLACE -> 'leaving' LOC\n",
            "    PP_PLACE -> 'to' LOC\n",
            "    PP_PLACE -> 'arriving' 'at' LOC\n",
            "    PP_TIME -> 'arriving' TIME\n",
            "    PP_TIME -> 'departing' TIME\n",
            "    PP_TIME -> 'leaving' TIME\n",
            "    LOC -> 'Boston'\n",
            "    LOC -> 'New' 'York'\n",
            "    LOC -> 'Tel' 'Aviv'\n",
            "    TIME -> 'in' 'the' 'morning'\n",
            "    TIME -> 'in' 'the' 'evening'\n"
          ]
        }
      ],
      "source": [
        "print(grammar)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c8a2bff",
      "metadata": {
        "id": "0c8a2bff"
      },
      "source": [
        "There are several things to notice about this grammar.\n",
        "\n",
        "1. We use the `parse_augmented_grammar` function provided by `scripts.transform` (which we downloaded during the setup), to provide a more pleasant format for specifying grammars and augmented grammars. This format allow for blank lines and comment lines, separating alternatives on separate lines, and (although we haven't used it yet) adding semantic augmentations to the syntactic rules.\n",
        "\n",
        "1. The grammar mixes familiar nonterminals like parts of speech and phrases (NP for noun phrases, PP for prepositional phrase) with nonterminals of a more semantic flavor, like LOC for location or TIME for time. Grammars with semantics-based nonterminals are often referred to as \"semantic grammars\". But, the grammar is still syntactic in the sense that it operates on expressions to provide their structure.\n",
        "\n",
        "1. The function returns two values, a grammar in standard NLTK format, and a dictionary storing the augmentations, about which more later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f74fa4",
      "metadata": {
        "id": "67f74fa4"
      },
      "source": [
        "### Augmenting the grammar with semantic composition functions\n",
        "\n",
        "Next, we will augment this grammar with rule meanings to construct logical expressions. For each production in the grammar, we'll provide a function that takes as arguments the meanings of the subconstituents (one for each nonterminal on the right-hand side of the production) and returns the meaning of the full constituent. For those rules that have no right-hand side nonterminals, their meaning will thus be a function that takes no arguments (in Python, written as `lambda: ...`).\n",
        "\n",
        "Before going further, make sure you fully understand this idea. The idea behind compositional semantics systems like this is that **the meaning of a constituent is determined by (is a _function_ of) the meanings of its subconstituents**. We'll call these functions _composition functions_. In our implementation, we are taking this _literally_, by having the function of the subconstituent meaning be an _actual Python function_. Some perhaps surprising things follow:\n",
        "\n",
        "1. Since the meanings might themselves be functions, these composition functions may themselves return functions.\n",
        "2. Since the subconstituent meanings might themselves be functions, these composition functions may take functions as arguments.\n",
        "3. Since there may be zero subconstiuents of a constituent, these composition functions may take no arguments at all.\n",
        "\n",
        "For instance, trees admitted by the syntactic production `NP -> 'flights'` have no (nontrivial) subconstituents. The production might have a composition function `lambda: lambda x: x in Flight`. Similarly, the production `TIME -> 'in' 'the' 'morning'` might have a semantic composition function `lambda: Morning`.\n",
        "\n",
        "We add augmentations to the grammar by placing them on the same line as the syntactic rule they augment, after a colon (`:`). Here we've added a few augmentations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4ad35340",
      "metadata": {
        "id": "4ad35340"
      },
      "outputs": [],
      "source": [
        "grammar_spec_1 = \"\"\"\n",
        "    NP -> 'flights'                 : lambda: lambda x: x in Flight\n",
        "    NP -> NP PP\n",
        "\n",
        "    PP -> PP_PLACE\n",
        "    PP -> PP_TIME\n",
        "\n",
        "    PP_PLACE -> 'from' LOC\n",
        "              | 'leaving' LOC\n",
        "              | 'to' LOC\n",
        "              | 'arriving' 'at' LOC\n",
        "\n",
        "    PP_TIME -> 'arriving' TIME\n",
        "             | 'departing' TIME\n",
        "             | 'leaving' TIME\n",
        "\n",
        "    LOC -> 'Boston'\n",
        "    LOC -> 'New' 'York'\n",
        "    LOC -> 'Tel' 'Aviv'\n",
        "\n",
        "    TIME -> 'in' 'the' 'morning'    : lambda: Morning\n",
        "    TIME -> 'in' 'the' 'evening'    : lambda: Evening\n",
        "\"\"\"\n",
        "\n",
        "grammar_1, augmentations_1 = xform.parse_augmented_grammar(grammar_spec_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "952708e5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "952708e5"
      },
      "source": [
        "Now it's your turn to add augmentations for the other productions that have no nonterminals on the right-hand side (just those for now).\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: q_grammar_spec_2\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "905cdfb8",
      "metadata": {
        "id": "905cdfb8"
      },
      "outputs": [],
      "source": [
        "## TODO - copy grammar_spec_1 from above and add augmentations\n",
        "#         for productions with no nonterminals on the right-hand side\n",
        "# Note: do not change the order of productions!\n",
        "grammar_spec_2 = \"\"\"\n",
        "    NP -> 'flights'                 : lambda: lambda x: x in Flight\n",
        "    NP -> NP PP\n",
        "\n",
        "    PP -> PP_PLACE\n",
        "    PP -> PP_TIME\n",
        "\n",
        "    PP_PLACE -> 'from' LOC\n",
        "              | 'leaving' LOC\n",
        "              | 'to' LOC\n",
        "              | 'arriving' 'at' LOC\n",
        "\n",
        "    PP_TIME -> 'arriving' TIME\n",
        "             | 'departing' TIME\n",
        "             | 'leaving' TIME\n",
        "\n",
        "    LOC -> 'Boston'                  : lambda: Boston\n",
        "    LOC -> 'New' 'York'              : lambda: NewYork\n",
        "    LOC -> 'Tel' 'Aviv'              : lambda: TelAviv\n",
        "\n",
        "    TIME -> 'in' 'the' 'morning'    : lambda: Morning\n",
        "    TIME -> 'in' 'the' 'evening'    : lambda: Evening\n",
        "\"\"\"\n",
        "\n",
        "grammar_2, augmentations_2 = xform.parse_augmented_grammar(grammar_spec_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f5e42bfe",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "f5e42bfe",
        "outputId": "b0bbb5b1-39f2-4c51-a834-99e99d032bdf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "grader.check(\"q_grammar_spec_2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c52548a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3c52548a"
      },
      "source": [
        "What about the rule `PP_TIME -> 'arriving' TIME`? A phrase like \"arriving in the morning\" ought to be associated with a Python expression that is true of things that arrive in the morning, that is, `lambda x: (x, Morning) in ArrivalTime`. We can work backwards from there.\n",
        "\n",
        "We know that the composition function for the rule will be a function of one argument, the meaning of the `TIME` subconstituent \"in the morning\", which we've already determined to be `Morning`.\n",
        "\n",
        "(Ask yourself, why isn't it `lambda: Morning`? Make sure you understand why before moving on.)\n",
        "\n",
        "Thus, the augmentation will be of the form `lambda Time: ...`, which will end up being applied to `Morning`, so that `Time` in this particular case will end up being `Morning`. What should you fill in for the `...` so that the result of the application will be `lambda x: (x, Morning) in ArrivalTime`?\n",
        "\n",
        "Add an augmentation for the `PP_TIME -> 'arriving' TIME` rule to the grammar based on your solution.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: q_grammar_spec_3\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "9abe86d9",
      "metadata": {
        "id": "9abe86d9"
      },
      "outputs": [],
      "source": [
        "## TODO - copy grammar_spec_2 from above and add augmentations\n",
        "#         for production PP_TIME -> 'arriving' TIME\n",
        "# Note: do not change the order of productions!\n",
        "grammar_spec_3 = \"\"\"\n",
        "    NP -> 'flights'                 : lambda: lambda x: x in Flight\n",
        "    NP -> NP PP\n",
        "\n",
        "    PP -> PP_PLACE\n",
        "    PP -> PP_TIME\n",
        "\n",
        "    PP_PLACE -> 'from' LOC\n",
        "              | 'leaving' LOC\n",
        "              | 'to' LOC\n",
        "              | 'arriving' 'at' LOC\n",
        "\n",
        "    PP_TIME -> 'arriving' TIME       : lambda time: lambda x: (x, time) in ArrivalTime\n",
        "             | 'departing' TIME\n",
        "             | 'leaving' TIME\n",
        "\n",
        "    LOC -> 'Boston'                  : lambda: Boston\n",
        "    LOC -> 'New' 'York'              : lambda: NewYork\n",
        "    LOC -> 'Tel' 'Aviv'              : lambda: TelAviv\n",
        "\n",
        "    TIME -> 'in' 'the' 'morning'    : lambda: Morning\n",
        "    TIME -> 'in' 'the' 'evening'    : lambda: Evening\n",
        "\"\"\"\n",
        "grammar_3, augmentations_3 = xform.parse_augmented_grammar(grammar_spec_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "62b01bc6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "62b01bc6",
        "outputId": "64fb9a31-f7cb-4643-fc8e-0dac671921e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "grader.check(\"q_grammar_spec_3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "201b1f5c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "201b1f5c"
      },
      "source": [
        "Once you get that augmentation in place, many others should be straightforward. Fill in augmentations for all of the `PP_PLACE -> *` and `PP_TIME -> *` productions.\n",
        "\n",
        "And what about the productions `PP -> PP_PLACE` and `PP -> PP_TIME`? Their composition functions are simple, since the meaning of the `PP` is just the same as the meaning of its right-hand side element. What composition function can achieve that? Fill in the augmentations for those rules too.\n",
        "\n",
        "Finally, the trickiest case is the composition function for the `NP -> NP PP` rule. Since it has two nonterminals on the right-hand side, it should be a function of two arguments, that is, something like `lambda NP, PP: ...`. This function will be applied to the meanings of the two subconstituents. The meanings for its two subconstituents, the NP and the PP, are each themselves going to be a function specifying a kind of property (like \"being a flight\" (`lambda x: x in Flight`) or \"originating in New York\" (`lambda x: (x, NewYork) in Origin`)). For the full constituent, its meaning should also be a property, namely the conjunction of the NP- and PP-provided properties (\"being a flight and originating in New York\" (`lambda x: x in Flight and (x, NewYork) in Origin`)). Define an appropriate augmentation that can do that as well, and add it to the grammar to complete the semantic augmentations.\n",
        "\n",
        "> **Hint:** Consider what should the composition function for `NP -> NP PP` return. Should it be a function? If so, how many arguments should it take?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: q_grammar_spec_4\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "09a9d6cc",
      "metadata": {
        "id": "09a9d6cc"
      },
      "outputs": [],
      "source": [
        "## TODO - copy grammar_spec_3 from above and add augmentations\n",
        "#         for productions PP_PLACE -> *, PP_TIME -> *,\n",
        "#         PP -> PP_PLACE, PP -> PP_TIME, and NP -> NP PP\n",
        "# Note: Do not change the order of productions!\n",
        "grammar_spec_4 = \"\"\"\n",
        "    NP -> 'flights'                   : lambda: lambda x: x in Flight\n",
        "    NP -> NP PP                       : lambda np, pp: lambda x: np(x) and pp(x)\n",
        "\n",
        "    PP -> PP_PLACE                    : lambda place: place\n",
        "    PP -> PP_TIME                     : lambda time: time\n",
        "\n",
        "    PP_PLACE -> 'from' LOC            : lambda loc: lambda x : (x, loc) in Origin\n",
        "              | 'leaving' LOC         : lambda loc: lambda x: (x, loc) in Origin\n",
        "              | 'to' LOC              : lambda loc: lambda x: (x, loc) in Destination\n",
        "              | 'arriving' 'at' LOC   : lambda loc: lambda x: (x, loc) in Destination\n",
        "\n",
        "    PP_TIME -> 'arriving' TIME        : lambda time: lambda x: (x, time) in ArrivalTime\n",
        "             | 'departing' TIME       : lambda time: lambda x: (x, time) in DepartureTime\n",
        "             | 'leaving' TIME         : lambda time: lambda x: (x, time) in DepartureTime\n",
        "\n",
        "    LOC -> 'Boston'                   : lambda: Boston\n",
        "    LOC -> 'New' 'York'               : lambda: NewYork\n",
        "    LOC -> 'Tel' 'Aviv'               : lambda: TelAviv\n",
        "\n",
        "    TIME -> 'in' 'the' 'morning'      : lambda: Morning\n",
        "    TIME -> 'in' 'the' 'evening'      : lambda: Evening\n",
        "\"\"\"\n",
        "grammar_4, augmentations_4 = xform.parse_augmented_grammar(grammar_spec_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "b3d03854",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "b3d03854",
        "outputId": "749303df-2a48-4f60-e367-8e944d318a10"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "grader.check(\"q_grammar_spec_4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2fe7949",
      "metadata": {
        "id": "f2fe7949"
      },
      "source": [
        "The `parse_augmented_grammar` function we've provided returns two values, an NLTK grammar based on the syntactic productions, and a dictionary that maps those productions onto the corresponding semantic augmentations. We can examine them individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "5d6c937f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d6c937f",
        "outputId": "4ceed03e-b13d-4265-aa11-234743572b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grammar with 16 productions (start state = NP)\n",
            "    NP -> 'flights'\n",
            "    NP -> NP PP\n",
            "    PP -> PP_PLACE\n",
            "    PP -> PP_TIME\n",
            "    PP_PLACE -> 'from' LOC\n",
            "    PP_PLACE -> 'leaving' LOC\n",
            "    PP_PLACE -> 'to' LOC\n",
            "    PP_PLACE -> 'arriving' 'at' LOC\n",
            "    PP_TIME -> 'arriving' TIME\n",
            "    PP_TIME -> 'departing' TIME\n",
            "    PP_TIME -> 'leaving' TIME\n",
            "    LOC -> 'Boston'\n",
            "    LOC -> 'New' 'York'\n",
            "    LOC -> 'Tel' 'Aviv'\n",
            "    TIME -> 'in' 'the' 'morning'\n",
            "    TIME -> 'in' 'the' 'evening'\n"
          ]
        }
      ],
      "source": [
        "print(grammar_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "aacfaffb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aacfaffb",
        "outputId": "46b45dcc-bb42-411f-c93c-9f3165706e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<function parse_augmented_grammar.<locals>.<lambda> at 0x7fceccb6c5e0>,\n",
            "            {LOC -> 'Boston': <function <lambda> at 0x7fcecc9440d0>,\n",
            "             LOC -> 'New' 'York': <function <lambda> at 0x7fcecc944b80>,\n",
            "             LOC -> 'Tel' 'Aviv': <function <lambda> at 0x7fcecc944ca0>,\n",
            "             NP -> 'flights': <function <lambda> at 0x7fcecc944040>,\n",
            "             NP -> NP PP: <function <lambda> at 0x7fceccb6c9d0>,\n",
            "             PP -> PP_PLACE: <function <lambda> at 0x7fcecc9445e0>,\n",
            "             PP -> PP_TIME: <function <lambda> at 0x7fcecc944550>,\n",
            "             PP_PLACE -> 'arriving' 'at' LOC: <function <lambda> at 0x7fcecc945a20>,\n",
            "             PP_PLACE -> 'from' LOC: <function <lambda> at 0x7fcecc944430>,\n",
            "             PP_PLACE -> 'leaving' LOC: <function <lambda> at 0x7fcecc944310>,\n",
            "             PP_PLACE -> 'to' LOC: <function <lambda> at 0x7fcecc9443a0>,\n",
            "             PP_TIME -> 'arriving' TIME: <function <lambda> at 0x7fcecc9441f0>,\n",
            "             PP_TIME -> 'departing' TIME: <function <lambda> at 0x7fcecc944160>,\n",
            "             PP_TIME -> 'leaving' TIME: <function <lambda> at 0x7fcecc945120>,\n",
            "             TIME -> 'in' 'the' 'evening': <function <lambda> at 0x7fcecc944e50>,\n",
            "             TIME -> 'in' 'the' 'morning': <function <lambda> at 0x7fcecc944ee0>})\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(augmentations_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "763b7608",
      "metadata": {
        "id": "763b7608"
      },
      "source": [
        "The semantic functions aren't much to look at. Python doesn't print out very useful information about them. But you can test them to see if they do the right thing. For instance, the semantic function for the `PP -> PP_PLACE` production ought to just be the identity function (something like `lambda PP_PLACE: PP_PLACE`).\n",
        "\n",
        "> A typical alternative that comes up is to make explicit the function status of the `PP_PLACE` meaning by apply the `PP_PLACE` meaning to a variable, say, `x`. To generate a function of the right type for the `PP` meaning, we'd need to reabstract over `x`, resulting in the `PP` meaning `lambda x: PP_PLACE(x)`. If you took this approach, providing a composition function for the rule of the form `lambda PP_PLACE: lambda x: PP_PLACE(x)`, know that these two `PP` meaning options â€“ `PP_PLACE` and `lambda x: PP_PLACE(x)` â€“ are, in an appropriate sense, equivalent. In the lambda calculus, their equivalence is codified in [the $\\eta$ rule](https://en.wikipedia.org/wiki/Lambda_calculus#Î·-reduction). You'll want to change to the simpler form for the tests below.\n",
        "\n",
        "Let's check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "d781271d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d781271d",
        "outputId": "7620cbd0-bc92-4f96-d4dc-48da157a02e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PP -> PP_PLACE"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "pp_place_production = list(augmentations_4.keys())[2]\n",
        "pp_place_production"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "f19fbe07",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f19fbe07",
        "outputId": "50724162-a9ee-476e-b7db-9b92f82619d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function transform.<lambda>(place)>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "pp_place_augmentation = augmentations_4[pp_place_production]\n",
        "pp_place_augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "bb55a645",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb55a645",
        "outputId": "31b1d1b3-270b-4654-b0f1-0cd4b457b3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n",
            "hello\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "for x in [42, 'hello', True]:\n",
        "  print(pp_place_augmentation(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c127e3",
      "metadata": {
        "id": "c8c127e3"
      },
      "source": [
        "So it sure looks like the augmentation for the `PP -> Place` rule is doing what we asked. (If not, check over your solution to the augmented grammar.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9628f3f",
      "metadata": {
        "id": "a9628f3f"
      },
      "source": [
        "## Applying the grammar\n",
        "\n",
        "With augmented grammar in hand, we can use it to get a FOL meaning representation for sentences. The procedure has two steps:\n",
        "\n",
        "1. Run a syntactic parsing algorithm to get a syntactic tree.\n",
        "2. Follow the tree derivation to obtain a meaning representation.\n",
        "\n",
        "The first step you're familiar with from the last segment of the course; this is the role of parsing algorithms such as CKY.\n",
        "\n",
        "The second step works by walking the tree, recursively constructing meanings for the subconstituents of a node, and then combining those subconstituent meanings by applying the production augmentation for the node to the subconstituent meanings to construct the meaning of the tree itself. This recursive method bottoms up when we come to a tree that has no nonterminal subconstituents; we just apply its production augmentation to the empty set of arguments.\n",
        "\n",
        "You could implement such a function â€“ in fact, you will in project segment 4 â€“ but for purpose of the lab today, you'll just carry out this process by hand.\n",
        "\n",
        "Let us walk through a semantic parsing of the expression \"flights from Boston\". First, we construct a syntactic parse tree for this sentence. The following function makes an `nltk` syntactic grammar from the syntactic-semantic grammar:\n",
        "\n",
        "We can use `nltk`'s `BottomUpChartParser` to parse the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "4833ebfb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4833ebfb",
        "outputId": "b0fc6a50-59dd-4546-b329-8a264b01e22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         NP                 \n",
            "    _____|______             \n",
            "   |            PP          \n",
            "   |            |            \n",
            "   |         PP_PLACE       \n",
            "   |      ______|_______     \n",
            "   NP    |             LOC  \n",
            "   |     |              |    \n",
            "flights from          Boston\n",
            "\n"
          ]
        }
      ],
      "source": [
        "parser = nltk.parse.BottomUpChartParser(grammar_4)\n",
        "\n",
        "sentence = \"flights from Boston\".split()\n",
        "for tree in [p for p in parser.parse(sentence)]:\n",
        "    tree.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "182de450",
      "metadata": {
        "id": "182de450"
      },
      "source": [
        "Below, we use Python variables like `A__some_words` to store the meaning for the constituent with nonterminal \"A\" spanning \"some words\".   \n",
        "\n",
        "For example, for the subtree `(NP 'flights')` at the bottom left of the tree, we'll store its meaning in the variable `NP__flights`. That meaning is constructed by applying the augmentation for the production `NP -> 'flights'` to the empty set of arguments. Hopefully, in the grammars starting with `grammar_spec_1` above, you've had the augmentation for that rule as `lambda: lambda x: x in Flight`. Applying this to the empty set of arguments, we get `(lambda: lambda x: x in Flight)()`, that is, `lambda x: x in Flight`.\n",
        "We record this for purposes of the lab as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "71303903",
      "metadata": {
        "id": "71303903"
      },
      "outputs": [],
      "source": [
        "NP__flights = (lambda: lambda x: x in Flight)()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0d61c0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2b0d61c0"
      },
      "source": [
        "Now do the same for the subtree rooted in `LOC`.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: derivation_1\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "c641cd50",
      "metadata": {
        "id": "c641cd50"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "LOC__Boston = (lambda: Boston)()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "1f4b5f76",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "1f4b5f76",
        "outputId": "a8939e0c-de2c-4c91-dfee-47e4c6eeafe6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "grader.check(\"derivation_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189f4bd5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "189f4bd5"
      },
      "source": [
        "Working bottom up, we consider the subtree rooted in `PP_PLACE`. What is the augmentation for the production used to form that subtree? Apply the augmentation to the meaning you've just computed for its one nonterminal-rooted subconstituent, which you've already stored as `LOC__Boston`, and call the result as `PP_PLACE__from_Boston`.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: derivation_2\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "ee27f917",
      "metadata": {
        "id": "ee27f917"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "PP_PLACE__from_boston = (lambda loc: lambda x : (x, loc) in Origin)(LOC__Boston)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "bc8a9d8a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "bc8a9d8a",
        "outputId": "d1fc7817-4b9d-4160-877b-6e2ded283982"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "grader.check(\"derivation_2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99f890ea",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "99f890ea"
      },
      "source": [
        "Again working bottom up, define the meaning for PP, by applying the meaning of the rule `PP -> PP_PLACE` to the meaning of the `PP_PLACE` just computed.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: derivation_3\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "22e1eba4",
      "metadata": {
        "id": "22e1eba4"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "PP__from_boston = (lambda place: place)(PP_PLACE__from_boston)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "7324f52a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "7324f52a",
        "outputId": "7200a4af-8eba-4c99-f27f-c79babd62f51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "grader.check(\"derivation_3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da37a408",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "da37a408"
      },
      "source": [
        "Finally, derive the meaning of the entire expression by applying the meaning of the rule `NP -> NP PP` to the meanings of its parts.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: derivation_4\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "df77c335",
      "metadata": {
        "id": "df77c335"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "NP__flights_from_boston = (lambda np, pp: lambda x: np(x) and pp(x))(NP__flights,PP__from_boston)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "5adfb526",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "5adfb526",
        "outputId": "af9be1b4-8193-4e41-b477-48e1d4caae23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "grader.check(\"derivation_4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a3a5d59",
      "metadata": {
        "id": "7a3a5d59"
      },
      "source": [
        "To check that you've got the correct expression, we run through all objects in the flight world and apply the expression to them, printing out the ones that evaluate to `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "cf15d8f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf15d8f5",
        "outputId": "67dd4f76-c1a0-440f-b1cc-a2f12afd6fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DL10', 'DL11']\n"
          ]
        }
      ],
      "source": [
        "print([obj for obj in Object if NP__flights_from_boston(obj)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f18afc6",
      "metadata": {
        "id": "8f18afc6"
      },
      "source": [
        "If your grammar augmentations are correct and you've carried out the deribvation correctly, you should get a list of just those objects that are flights from Boston, namely, DL10 and DL11.\n",
        "\n",
        "Let's move to a more complex example. Now that you know the drill, construct the meaning for \"flights from Boston to New York\" using the same augmented grammar. Here's the parse tree:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "9a9aff71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9aff71",
        "outputId": "9519cbfc-9df0-444a-f400-23353a1a7549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        NP                        \n",
            "          ______________|___________               \n",
            "         NP                         |             \n",
            "    _____|______                    |              \n",
            "   |            PP                  PP            \n",
            "   |            |                   |              \n",
            "   |         PP_PLACE            PP_PLACE         \n",
            "   |      ______|_______       _____|______        \n",
            "   NP    |             LOC    |           LOC     \n",
            "   |     |              |     |      ______|___    \n",
            "flights from          Boston  to   New        York\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sentence = \"flights from Boston to New York\".split()\n",
        "parses = [p for p in parser.parse(sentence)]\n",
        "for tree in parses:\n",
        "    tree.pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd7a8c3a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "cd7a8c3a"
      },
      "source": [
        "Create the meaning representation by walking through the tree bottom up. You can use the result you got for \"flights from Boston\" and compose it with the rest of the expression. The result will be stored as `NP__flights_from_boston_to_new_york`.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: derivation_5\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "52a2fece",
      "metadata": {
        "id": "52a2fece"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "LOC__NewYork = (lambda: NewYork)()\n",
        "PP_PLACE__to_new_york = (lambda loc: lambda x : (x, loc) in Destination)(LOC__NewYork)\n",
        "PP__to_new_york = (lambda place: place)(PP_PLACE__to_new_york)\n",
        "NP__flights_from_boston_to_new_york = (lambda np, pp: lambda x: np(x) and pp(x))(NP__flights_from_boston,PP__to_new_york)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "2291e4d3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "2291e4d3",
        "outputId": "b08a0fc5-cc2d-46fa-8c5f-2aed299e8e57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "grader.check(\"derivation_5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79f815d1",
      "metadata": {
        "id": "79f815d1"
      },
      "source": [
        "Now run the expression against the flight world to verify you got the correct result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "ce944115",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce944115",
        "outputId": "20b5ff75-e444-4f90-9d27-645140332aa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['DL10']\n"
          ]
        }
      ],
      "source": [
        "print([obj for obj in Object if NP__flights_from_boston_to_new_york(obj)])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5e95118",
      "metadata": {
        "id": "a5e95118"
      },
      "source": [
        "# Scaling up\n",
        "\n",
        "Constructing the meaning representation manually by traversing a tree is a tedious process. In practice, we would like an automated process, which, given any syntactic-semantic grammar and a tree consistent with the syntactic grammar returns a semantic representation. So far, we've seen how to use the syntactic parse tree to guide the composition of semantic representations. In project segment 4, you will automate this process by implementing such a generic semantic parser, which takes any tree with associated semantic rules, and constructs the meaning representation, doing just the work you've been doing by hand here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccbd8af7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ccbd8af7"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on might include the following:\n",
        "\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab?\n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were?\n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "but you should comment on whatever aspects you found especially positive or negative.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4528b79d",
      "metadata": {
        "id": "4528b79d"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4a9a766",
      "metadata": {
        "id": "a4a9a766"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of Lab 4-1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad695552",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ad695552"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d21e49c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "0d21e49c"
      },
      "outputs": [],
      "source": [
        "grader.check_all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS236299 Lab 4-1: First-order logic, lambda calculus, semantic parsing",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}